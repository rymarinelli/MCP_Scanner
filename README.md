# MCP Scanner

Foundational scaffolding for the MCP Scanner project. The repository currently
contains shared configuration, logging helpers, and abstract interfaces for
graph, LLM, and scanning components.

## Prerequisites

- Python 3.10+
- `pip`

## Getting started

1. **Create a virtual environment**

   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows use: .venv\\Scripts\\activate
   ```

2. **Install the project in editable mode**

   ```bash
   pip install --upgrade pip
   pip install -e .[dev]
   ```

3. **Provide configuration (optional)**

   Copy `.env.example` to `.env` and adjust any values as needed.

   ```bash
   cp .env.example .env
   ```

   Environment variables are prefixed with `MCP_` and can be nested using
   double underscores. For example, `MCP_LLM__API_KEY` sets the LLM provider
   API key.

## Project layout

```text
src/
├── common/        # Shared helpers (config, logging, etc.)
├── graph/         # Graph provider abstractions
├── llm/           # LLM provider abstractions
└── scanners/      # Repository scanner interfaces
```

## Loading configuration

Pydantic settings are used to centralize runtime configuration:

```python
from common.config import get_settings

settings = get_settings()
print(settings.llm.model)
```

The loader reads from `.env` by default and falls back to environment
variables.

## Using the bundled Hugging Face coding model

The remediation pipeline can run entirely on CPU by enabling the local
Hugging Face integration. Set the following environment variables to make the
service load the `ise-uiuc/Magicoder-S-DS-6.7B` model (or another compatible
coding model) via the Transformers library:

```bash
export MCP_LLM__PROVIDER=huggingface
export MCP_LLM__MODEL=ise-uiuc/Magicoder-S-DS-6.7B
```

When these settings are active, the remediation suggester streams prompts to
the locally hosted model instead of relying on a remote API. You can substitute
`MCP_LLM__MODEL` for a smaller CPU-friendly model (for example,
`deepseek-ai/deepseek-coder-1.3b-instruct`) if memory is constrained. The first
run will download the model weights to the local Hugging Face cache.

## Running the HTTP service via Docker

The repository now includes a lightweight HTTP service that exposes the
scanner workflow. Build and run the container as shown below:

```bash
docker build -t mcp-scanner .
docker run --rm -p 8000:8000 mcp-scanner
```

Trigger a scan by sending a `POST` request to the `/scan` endpoint. Provide
the Git URL (and optional branch) in the request body:

```bash
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"repo_url": "https://github.com/example/repo.git", "branch": "main"}' \
  http://localhost:8000/scan
```

The response contains the Semgrep findings and proposed remediation patches
generated by the MCP pipeline.

## Exposing the service with mytunnel (localtunnel)

When you need to share your local MCP Scanner instance with remote clients,
use the bundled test deployment helper. The script launches the HTTP server,
ensures the [`mytunnel`](https://www.npmjs.com/package/localtunnel) CLI is
available, and prints the public URL emitted by the tunnel:

```bash
python -m mcp_scanner.scripts.deploy_test_tunnel --install-lt --subdomain busy-papers-melt
```

Key flags:

- `--install-lt` installs the `lt` CLI with `npm` if it is not already
  available. Omit it when running in environments where global installs are not
  desired.
- `--subdomain` requests a stable localtunnel subdomain.
- `--host` and `--port` control the embedded MCP server binding (default:
  `0.0.0.0:8000`).

The command keeps running until interrupted with `Ctrl+C`, shutting down both
the server and tunnel cleanly when you exit.
